{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWtK6w_yeHgz"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/aya_vision_intro.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFp9yLxGnKLP"
      },
      "source": [
        "# Introduction to Aya Vision: A state-of-the-art open-weights vision model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqepQUilnKLQ"
      },
      "source": [
        "Introducing Aya Vision - a state-of-the-art open-weights multimodal multilingual model.\n",
        "\n",
        "In this notebook, we will explore the capabilities of Aya Vision, which can take text and image inputs to generates text responses.\n",
        "\n",
        "The following links provide further details about the Aya Vision model:\n",
        "- [The launch blog](https://cohere.com/blog/aya-vision)\n",
        "- [Documentation](https://docs.cohere.com/docs/aya-multimodal)\n",
        "- HuggingFace model page for the [32B](https://huggingface.co/CohereForAI/aya-vision-32b) and [8B](https://huggingface.co/CohereForAI/aya-vision-8b) models.\n",
        "\n",
        "This tutorial will provide a walkthrough of the various use cases that you can build with Aya Vision. By the end of this notebook, you will have a solid understanding of how to use Aya Vision for a wide range of applications.\n",
        "\n",
        "The list of possible use cases with multimodal models is endless, but this notebook will cover the following:\n",
        "- Setup\n",
        "- Question answering\n",
        "- Multilingual multimodal understanding\n",
        "- Captioning\n",
        "- Recognizing text\n",
        "- Classification\n",
        "- Comparing multiple images\n",
        "- Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVzdehb_nKLQ"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKcMKL5znyOS"
      },
      "source": [
        "First, install the Cohere Python SDK and create a client.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RWXA6_QSnlv0"
      },
      "outputs": [],
      "source": [
        "%pip install cohere -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "54DRcTVmnleu"
      },
      "outputs": [],
      "source": [
        "import cohere\n",
        "import base64\n",
        "\n",
        "co = cohere.ClientV2(\n",
        "    \"COHERE_API_KEY\"\n",
        ")  # Get your free API key here: https://dashboard.cohere.com/api-keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSxf8oxxnKLR"
      },
      "source": [
        "Next, let's set up a function to generate text responses, given an image and a message. It uses the Cohere API via the Chat endpoint to call the Aya Vision model.\n",
        "\n",
        "To pass an image to the API, pass a Base64-encoded image as the `image_url` argument in the `messages` parameter. To convert and image into a Base64-encoded version, we can use the `base64` library as in this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "d0jb1FYZnKLR"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "model=\"c4ai-aya-vision-32b\"\n",
        "\n",
        "def generate_text(image_path, message):\n",
        "    \"\"\"\n",
        "    Generate text responses from Aya Vision model based on an image and text prompt.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file\n",
        "        message (str): Text prompt to send with the image\n",
        "\n",
        "    Returns:\n",
        "        None: Prints the model's response\n",
        "    \"\"\"\n",
        "\n",
        "    # Define an image in Base64-encoded format\n",
        "    with open(image_path, \"rb\") as img_file:\n",
        "        base64_image_url = f\"data:image/jpeg;base64,{base64.b64encode(img_file.read()).decode('utf-8')}\"\n",
        "\n",
        "    # Make an API call to the Cohere Chat endpoint, passing the user message and image\n",
        "    response = co.chat(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": message},\n",
        "                    {\"type\": \"image_url\", \"image_url\": {\"url\": base64_image_url}},\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Print the response\n",
        "    print(response.message.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl8qbzMqnKLR"
      },
      "source": [
        "Let's also set up a function to render images on this notebook as we go through the use cases.\n",
        "\n",
        "Note: the images used in this notebook can be downloaded [here](https://github.com/cohere-ai/cohere-developer-experience/tree/main/notebooks/images/aya-vision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "S7HaotS2nKLR"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "def render_image(image_path):\n",
        "    \"\"\"\n",
        "    Display an image in the notebook with a fixed width.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file to display\n",
        "    \"\"\"\n",
        "    display(Image(filename=image_path, width=400))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHSxcIHbnKLR"
      },
      "source": [
        "## Question answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh9thY1qnKLR"
      },
      "source": [
        "One of the more common use cases is question answering. Here, the model is used to answer questions based on the content of an image.\n",
        "\n",
        "By providing an image and a relevant question, the model can analyze the visual content and generate a text response. This is particularly useful in scenarios where visual context is important, such as identifying objects, understanding scenes, or providing descriptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "gtFRD_-JnKLR",
        "outputId": "07215071-c0cc-4b98-c61c-d7924fe393e1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'https://static.vecteezy.com/system/resources/thumbnails/036/324/708/small/ai-generated-picture-of-a-tiger-walking-in-the-forest-photo.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e5050a78e59e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://static.vecteezy.com/system/resources/thumbnails/036/324/708/small/ai-generated-picture-of-a-tiger-walking-in-the-forest-photo.jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrender_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-e10586d51ca7>\u001b[0m in \u001b[0;36mrender_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mimage_path\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mimage\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[0m\u001b[1;32m   1232\u001b[0m                 metadata=metadata)\n\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https://static.vecteezy.com/system/resources/thumbnails/036/324/708/small/ai-generated-picture-of-a-tiger-walking-in-the-forest-photo.jpg'"
          ]
        }
      ],
      "source": [
        "image_path = \"image1.jpg\"\n",
        "render_image(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGBDwX45nKLR"
      },
      "outputs": [],
      "source": [
        "message = \"Where is this art style from and what is this dish typically used for?\"\n",
        "generate_text(image_path, message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnVBZmKanKLR"
      },
      "source": [
        "## Multilingual multimodal understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vA6RQtWnKLR"
      },
      "source": [
        "Aya Vision can process and respond to prompts in multiple languages, demonstrating its multilingual capabilities. This feature allows users to interact with the model in their preferred language, making it accessible to a global audience. The model can analyze images and provide relevant responses based on the visual content, regardless of the language used in the query.\n",
        "\n",
        "Here is an example in Persian:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68jDsOYOnKLR"
      },
      "outputs": [],
      "source": [
        "image_path = \"image2.jpg\"\n",
        "render_image(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGOIXqoZnKLR"
      },
      "outputs": [],
      "source": [
        "message = \"آیا این یک هدیه مناسب برای یک کودک 3 ساله است؟\"\n",
        "generate_text(image_path, message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Azwh8PhnKLS"
      },
      "source": [
        "And here's an example in Indonesian:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvNw7JI9nKLS"
      },
      "outputs": [],
      "source": [
        "image_path = \"image3.jpg\"\n",
        "render_image(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBGGefydnKLS"
      },
      "outputs": [],
      "source": [
        "message = \"Gambar ini berisikan kutipan dari tokoh nasional di Indonesia, siapakah tokoh itu?\"\n",
        "generate_text(image_path, message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxujtczqnKLS"
      },
      "source": [
        "## Captioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1ErHcYGnKLS"
      },
      "source": [
        "Instead of asking about specific questions, we can also get the model to provide a description of an image as a whole, be it detailed descriptions or simple captions.\n",
        "\n",
        "This can be particularly useful for creating alt text for accessibility, generating descriptions for image databases, social media content creation, and others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbIRrMC8nKLS"
      },
      "outputs": [],
      "source": [
        "image_path = \"image4.jpg\"\n",
        "render_image(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5o3kMo2hnKLS"
      },
      "outputs": [],
      "source": [
        "message = \"Describe this image in detail.\"\n",
        "\n",
        "generate_text(image_path, message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA29PlzmnKLS"
      },
      "source": [
        "## Recognizing text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceNKE11rnKLS"
      },
      "source": [
        "The model can recognize and extract text from images, which is useful for reading signs, documents, or other text-based content in photographs. This capability enables applications that can answer questions about text content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5UrG7wwnKLS"
      },
      "outputs": [],
      "source": [
        "image_path = \"image5.jpg\"\n",
        "render_image(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qbg6BYFYnKLS"
      },
      "outputs": [],
      "source": [
        "message = \"How many bread rolls do I get?\"\n",
        "\n",
        "generate_text(image_path, message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAAyWOhenKLS"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Aiu86CUnKLS"
      },
      "source": [
        "Classification allows the model to categorize images into predefined classes or labels. This is useful for organizing visual content, filtering images, or extracting structured information from visual data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWTypnqanKLS"
      },
      "outputs": [],
      "source": [
        "image_path1 = \"image6.jpg\"\n",
        "image_path2 = \"image7.jpg\"\n",
        "render_image(image_path1)\n",
        "render_image(image_path2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPfSMQdWnKLS"
      },
      "outputs": [],
      "source": [
        "message = \"Please classify this image as one of these dish types: japanese, malaysian, turkish, or other.Respond in the following format: dish_type: <the_dish_type>.\"\n",
        "\n",
        "images = [\n",
        "    image_path1, # turkish\n",
        "    image_path2, # japanese\n",
        "]\n",
        "\n",
        "for item in images:\n",
        "    generate_text(item, message)\n",
        "    print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K827fpkfnKLS"
      },
      "source": [
        "## Comparing multiple images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6Pa3xVxnKLS"
      },
      "source": [
        "This section demonstrates how to analyze and compare multiple images simultaneously. The API allows passing more than one image in a single call, enabling the model to perform comparative analysis between different visual inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMvxcnM9nKLS"
      },
      "outputs": [],
      "source": [
        "image_path1 = \"image6.jpg\"\n",
        "image_path2 = \"image7.jpg\"\n",
        "render_image(image_path1)\n",
        "render_image(image_path2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIQgKvwQnKLS"
      },
      "outputs": [],
      "source": [
        "message = \"Compare these two dishes.\"\n",
        "\n",
        "with open(image_path1, \"rb\") as img_file1:\n",
        "    base64_image_url1 = f\"data:image/jpeg;base64,{base64.b64encode(img_file1.read()).decode('utf-8')}\"\n",
        "\n",
        "with open(image_path2, \"rb\") as img_file2:\n",
        "    base64_image_url2 = f\"data:image/jpeg;base64,{base64.b64encode(img_file2.read()).decode('utf-8')}\"\n",
        "\n",
        "response = co.chat(\n",
        "    model=model,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": message},\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\": base64_image_url1}},\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\":base64_image_url2}}\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(response.message.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-ndAyjVnKLT"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcloy_U4nKLT"
      },
      "source": [
        "In this notebook, we've explored the capabilities of the Aya Vision model through various examples.\n",
        "\n",
        "The Aya Vision model shows impressive capabilities in understanding visual content and providing detailed, contextual responses. This makes it suitable for a wide range of applications including content analysis, accessibility features, educational tools, and more.\n",
        "\n",
        "The API's flexibility in handling different types of queries and multiple images simultaneously makes it a powerful tool if you are looking to integrate advanced computer vision capabilities into your applications."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}